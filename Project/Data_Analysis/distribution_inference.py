# -*- coding: utf-8 -*-
"""
å†œæ‘å¥³æ€§å°±ä¸šå¸‚åœºä¸»ä½“ç‰¹å¾åˆ†å¸ƒæ¨æ–­æ¨¡å—

æœ¬æ¨¡å—å®ç°åŸºäºæœ€å¤§ä¼¼ç„¶ä¼°è®¡(MLE)å’ŒAnderson-Darlingæ£€éªŒçš„æ¦‚ç‡åˆ†å¸ƒæ‹Ÿåˆåˆ†æï¼Œ
ä¸ºå†œæ‘å¥³æ€§å°±ä¸šå¸‚åœºæ•°æ®æä¾›ç†è®ºåˆ†å¸ƒæ¨¡å‹ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. å¤šç§æ¦‚ç‡åˆ†å¸ƒçš„å‚æ•°ä¼°è®¡ (MLE)
2. æ‹Ÿåˆä¼˜åº¦æ£€éªŒ (Anderson-Darling)
3. æ¨¡å‹é€‰æ‹©ä¸æ¯”è¾ƒ (AIC/BIC)
4. åˆ†å¸ƒæ¨æ–­ç»“æœæŠ¥å‘Šç”Ÿæˆ

Author: Claude-4 AI Assistant
Date: 2024-09-24
Version: 1.3.0
å¯¹åº”ç ”ç©¶è®¡åˆ’ç¬¬4.2èŠ‚ï¼šå¸‚åœºä¸»ä½“ç‰¹å¾çš„ç¡®å®š
"""

from typing import Dict, List, Optional, Tuple, Union, Any
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.optimize import minimize
import warnings

# é…ç½®è­¦å‘Šè¿‡æ»¤
warnings.filterwarnings('ignore', category=RuntimeWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œç»˜å›¾æ ·å¼
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
plt.style.use('seaborn-v0_8')

# ==================== å¸¸é‡å®šä¹‰ ====================
# æ•°å€¼è®¡ç®—å¸¸é‡
NUMERICAL_EPSILON = 1e-6          # æ•°å€¼ç¨³å®šæ€§å¸¸é‡ï¼Œé¿å…è¾¹ç•Œå€¼
LOGNORM_SHIFT = 0.1               # å¯¹æ•°æ­£æ€åˆ†å¸ƒé›¶å€¼å¤„ç†åç§»é‡  
MIN_SAMPLE_SIZE = 10              # æœ€å°æ ·æœ¬é‡è¦æ±‚
CLIP_EPSILON = 1e-10              # CDFè£å‰ªé˜ˆå€¼ï¼Œé¿å…æå€¼

# Anderson-Darlingç»Ÿè®¡é‡è°ƒæ•´å‚æ•°
AD_ADJUSTMENT_COEF1 = 0.75        # æ ·æœ¬é‡è°ƒæ•´ç³»æ•°1
AD_ADJUSTMENT_COEF2 = 2.25        # æ ·æœ¬é‡è°ƒæ•´ç³»æ•°2

# på€¼è®¡ç®—é˜ˆå€¼
P_VALUE_THRESHOLD_HIGH = 0.6      # é«˜é˜ˆå€¼
P_VALUE_THRESHOLD_MID = 0.34      # ä¸­é˜ˆå€¼  
P_VALUE_THRESHOLD_LOW = 0.2       # ä½é˜ˆå€¼

# æ”¯æŒçš„æ¦‚ç‡åˆ†å¸ƒæ—
SUPPORTED_DISTRIBUTIONS = {
    'norm': stats.norm,              # æ­£æ€åˆ†å¸ƒ
    'gamma': stats.gamma,            # ä¼½é©¬åˆ†å¸ƒ
    'expon': stats.expon,            # æŒ‡æ•°åˆ†å¸ƒ
    'weibull_min': stats.weibull_min, # å¨å¸ƒå°”åˆ†å¸ƒ
    'lognorm': stats.lognorm,        # å¯¹æ•°æ­£æ€åˆ†å¸ƒ
    'beta': stats.beta,              # è´å¡”åˆ†å¸ƒ
    'uniform': stats.uniform,        # å‡åŒ€åˆ†å¸ƒ
    'pareto': stats.pareto,          # å¸•ç´¯æ‰˜åˆ†å¸ƒ
    'chi2': stats.chi2,              # å¡æ–¹åˆ†å¸ƒ
    'genextreme': stats.genextreme   # å¹¿ä¹‰æå€¼åˆ†å¸ƒ
}

class DistributionFitter:
    """
    æ¦‚ç‡åˆ†å¸ƒæ‹Ÿåˆä¸ç»Ÿè®¡æ£€éªŒç±»
    
    è¯¥ç±»å®ç°äº†å¤šç§æ¦‚ç‡åˆ†å¸ƒçš„å‚æ•°ä¼°è®¡å’Œæ‹Ÿåˆä¼˜åº¦æ£€éªŒï¼Œä¸»è¦ç”¨äºå†œæ‘å¥³æ€§å°±ä¸šå¸‚åœºæ•°æ®çš„
    åˆ†å¸ƒå»ºæ¨¡ã€‚æ”¯æŒçš„åˆ†æåŒ…æ‹¬ï¼š
    - æœ€å¤§ä¼¼ç„¶ä¼°è®¡(MLE)å‚æ•°ä¼°è®¡
    - Anderson-Darlingæ‹Ÿåˆä¼˜åº¦æ£€éªŒ  
    - AIC/BICæ¨¡å‹é€‰æ‹©
    - ç»Ÿè®¡æ¨æ–­ç»“æœæ¯”è¾ƒ
    
    Attributes:
        distributions (Dict): æ”¯æŒçš„æ¦‚ç‡åˆ†å¸ƒå­—å…¸
        results (Dict): å­˜å‚¨å„å˜é‡çš„åˆ†å¸ƒæ‹Ÿåˆç»“æœ
        
    Example:
        >>> fitter = DistributionFitter()
        >>> fitter.fit_variable(data, 'å˜é‡å')
        >>> fitter.create_comparison_table()
    """
    
    def __init__(self) -> None:
        """
        åˆå§‹åŒ–åˆ†å¸ƒæ‹Ÿåˆå™¨
        
        ä½¿ç”¨é¢„å®šä¹‰çš„å¸¸é‡SUPPORTED_DISTRIBUTIONSåˆå§‹åŒ–æ”¯æŒçš„åˆ†å¸ƒæ—ï¼Œ
        å¹¶åˆ›å»ºç©ºçš„ç»“æœå­˜å‚¨å­—å…¸ã€‚
        """
        # ä½¿ç”¨å…¨å±€å¸¸é‡å®šä¹‰çš„åˆ†å¸ƒæ—
        self.distributions: Dict[str, Any] = SUPPORTED_DISTRIBUTIONS.copy()
        
        # å­˜å‚¨æ¯ä¸ªå˜é‡çš„æ‹Ÿåˆç»“æœ
        # ç»“æ„: {å˜é‡å: {åˆ†å¸ƒå: {å‚æ•°, ç»Ÿè®¡é‡, æŒ‡æ ‡}}}
        self.results: Dict[str, Dict[str, Dict[str, Any]]] = {}
    
    def mle_estimation(self, data: np.ndarray, dist_name: str) -> Optional[Tuple[float, ...]]:
        """
        æœ€å¤§ä¼¼ç„¶ä¼°è®¡ (MLE) å‚æ•°ä¼°è®¡
        
        å¯¹æŒ‡å®šçš„æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œæœ€å¤§ä¼¼ç„¶å‚æ•°ä¼°è®¡ã€‚é’ˆå¯¹ä¸åŒåˆ†å¸ƒç±»å‹çš„æ•°æ®è¦æ±‚ï¼Œ
        è¿›è¡Œç›¸åº”çš„é¢„å¤„ç†ï¼ˆå¦‚Betaåˆ†å¸ƒçš„æ ‡å‡†åŒ–ã€å¯¹æ•°æ­£æ€åˆ†å¸ƒçš„é›¶å€¼å¤„ç†ï¼‰ã€‚
        
        Args:
            data (np.ndarray): è¾“å…¥æ•°æ®æ•°ç»„
            dist_name (str): åˆ†å¸ƒåç§°ï¼Œå¿…é¡»åœ¨æ”¯æŒçš„åˆ†å¸ƒåˆ—è¡¨ä¸­
            
        Returns:
            Optional[Tuple[float, ...]]: ä¼°è®¡çš„å‚æ•°å…ƒç»„ï¼Œå¤±è´¥æ—¶è¿”å›None
            
        Note:
            - Betaåˆ†å¸ƒï¼šæ•°æ®æ ‡å‡†åŒ–åˆ°[0,1]åŒºé—´
            - å¯¹æ•°æ­£æ€åˆ†å¸ƒï¼šé›¶å€¼æ·»åŠ åç§»é‡é¿å…æ•°å€¼é—®é¢˜
            - å‡åŒ€åˆ†å¸ƒï¼šç›´æ¥ä½¿ç”¨æ•°æ®èŒƒå›´è®¡ç®—å‚æ•°
        """
        try:
            dist = self.distributions[dist_name]
            
            # ========== ç‰¹æ®Šåˆ†å¸ƒçš„å‚æ•°ä¼°è®¡å¤„ç† ==========
            if dist_name == 'beta':
                # Betaåˆ†å¸ƒï¼šæ ‡å‡†åŒ–åˆ°[0,1]åŒºé—´å¹¶é¿å…è¾¹ç•Œå€¼
                data_range = data.max() - data.min()
                if data_range == 0:  # å¤„ç†å¸¸æ•°æ•°æ®çš„æƒ…å†µ
                    return None
                data_scaled = (data - data.min()) / data_range
                data_scaled = np.clip(data_scaled, NUMERICAL_EPSILON, 1 - NUMERICAL_EPSILON)
                params = dist.fit(data_scaled, floc=0, fscale=1)
                
            elif dist_name == 'uniform':
                # å‡åŒ€åˆ†å¸ƒï¼šç›´æ¥ä½¿ç”¨æ•°æ®èŒƒå›´
                params = (data.min(), data.max() - data.min())
                
            elif dist_name == 'lognorm':
                # å¯¹æ•°æ­£æ€åˆ†å¸ƒï¼šæ·»åŠ åç§»é‡é¿å…é›¶å€¼å’Œè´Ÿå€¼
                data_shifted = data + LOGNORM_SHIFT
                params = dist.fit(data_shifted)
                
            elif dist_name == 'pareto':
                # å¸•ç´¯æ‰˜åˆ†å¸ƒï¼šå›ºå®šä½ç½®å‚æ•°ä¸º0
                params = dist.fit(data, floc=0)
                
            else:
                # æ ‡å‡†MLEä¼°è®¡ï¼ˆæ­£æ€ã€ä¼½é©¬ã€æŒ‡æ•°ç­‰ï¼‰
                params = dist.fit(data)
            
            return params
            
        except (ValueError, RuntimeError, np.linalg.LinAlgError) as e:
            print(f"    âŒ MLEä¼°è®¡å¤±è´¥ ({dist_name}): {str(e)}")
            return None
        except Exception as e:
            print(f"    âŒ MLEä¼°è®¡æœªçŸ¥é”™è¯¯ ({dist_name}): {str(e)}")
            return None
    
    def anderson_darling_test(self, data: np.ndarray, dist_name: str, 
                            params: Tuple[float, ...]) -> Tuple[Optional[float], Optional[float], Optional[float]]:
        """
        Anderson-Darlingæ‹Ÿåˆä¼˜åº¦æ£€éªŒ
        
        è®¡ç®—Anderson-Darlingç»Ÿè®¡é‡æ¥æ£€éªŒæ•°æ®æ˜¯å¦ç¬¦åˆæŒ‡å®šçš„æ¦‚ç‡åˆ†å¸ƒã€‚
        è¯¥æ£€éªŒå¯¹åˆ†å¸ƒçš„å°¾éƒ¨æ›´åŠ æ•æ„Ÿï¼Œé€‚ç”¨äºæ£€éªŒåˆ†å¸ƒæ‹Ÿåˆçš„è´¨é‡ã€‚
        
        Args:
            data (np.ndarray): åŸå§‹æ•°æ®æ•°ç»„
            dist_name (str): åˆ†å¸ƒåç§°
            params (Tuple[float, ...]): åˆ†å¸ƒå‚æ•°
            
        Returns:
            Tuple[Optional[float], Optional[float], Optional[float]]: 
                (åŸå§‹ADç»Ÿè®¡é‡, è°ƒæ•´åADç»Ÿè®¡é‡, på€¼)ï¼Œå¤±è´¥æ—¶è¿”å›(None, None, None)
                
        Note:
            - ADç»Ÿè®¡é‡è¶Šå°ï¼Œæ‹Ÿåˆè¶Šå¥½
            - på€¼>0.05é€šå¸¸è®¤ä¸ºæ¥å—åŸå‡è®¾ï¼ˆæ•°æ®ç¬¦åˆåˆ†å¸ƒï¼‰
            - é’ˆå¯¹ä¸åŒåˆ†å¸ƒç±»å‹è¿›è¡Œç›¸åº”çš„æ•°æ®é¢„å¤„ç†
        """
        try:
            dist = self.distributions[dist_name]
            n = len(data)
            
            # ========== æ ¹æ®åˆ†å¸ƒç±»å‹å‡†å¤‡æ•°æ® ==========
            if dist_name == 'beta':
                # Betaåˆ†å¸ƒï¼šæ ‡å‡†åŒ–å¤„ç†
                data_range = data.max() - data.min()
                if data_range == 0:
                    return None, None, None
                data_processed = (data - data.min()) / data_range
                data_processed = np.clip(data_processed, NUMERICAL_EPSILON, 1 - NUMERICAL_EPSILON)
                
            elif dist_name == 'lognorm':
                # å¯¹æ•°æ­£æ€åˆ†å¸ƒï¼šæ·»åŠ åç§»é‡
                data_processed = data + LOGNORM_SHIFT
                
            else:
                # å…¶ä»–åˆ†å¸ƒï¼šä½¿ç”¨åŸå§‹æ•°æ®
                data_processed = data.copy()
            
            # å¯¹æ•°æ®æ’åºå¹¶è®¡ç®—ç†è®ºCDFå€¼
            data_sorted = np.sort(data_processed)
            F = dist.cdf(data_sorted, *params)
            
            # é¿å…æå€¼ï¼Œç¡®ä¿æ•°å€¼ç¨³å®šæ€§
            F = np.clip(F, CLIP_EPSILON, 1 - CLIP_EPSILON)
            
            # ========== è®¡ç®—ADç»Ÿè®¡é‡ ==========
            i = np.arange(1, n + 1)
            # ADç»Ÿè®¡é‡æ ‡å‡†å…¬å¼
            ad_stat = -n - np.sum((2*i - 1) * (np.log(F) + np.log(1 - F[::-1]))) / n
            
            # æ ·æœ¬é‡è°ƒæ•´ï¼ˆæé«˜å°æ ·æœ¬çš„å‡†ç¡®æ€§ï¼‰
            ad_stat_adj = ad_stat * (1 + AD_ADJUSTMENT_COEF1/n + AD_ADJUSTMENT_COEF2/n**2)
            
            # ========== è®¡ç®—på€¼ï¼ˆè¿‘ä¼¼å…¬å¼ï¼‰ ==========
            p_value = self._calculate_ad_p_value(ad_stat_adj)
            
            return ad_stat, ad_stat_adj, p_value
            
        except (ValueError, RuntimeError, np.linalg.LinAlgError) as e:
            print(f"    âŒ ADæ£€éªŒè®¡ç®—å¤±è´¥ ({dist_name}): {str(e)}")
            return None, None, None
        except Exception as e:
            print(f"    âŒ ADæ£€éªŒæœªçŸ¥é”™è¯¯ ({dist_name}): {str(e)}")
            return None, None, None
    
    def _calculate_ad_p_value(self, ad_stat_adj: float) -> float:
        """
        è®¡ç®—Anderson-Darlingç»Ÿè®¡é‡çš„på€¼
        
        ä½¿ç”¨åˆ†æ®µè¿‘ä¼¼å…¬å¼è®¡ç®—på€¼ï¼ŒåŸºäºç»Ÿè®¡æ–‡çŒ®ä¸­çš„æ ‡å‡†æ–¹æ³•ã€‚
        
        Args:
            ad_stat_adj (float): è°ƒæ•´åçš„ADç»Ÿè®¡é‡
            
        Returns:
            float: på€¼ï¼ŒèŒƒå›´[0,1]
        """
        # æ ¹æ®ADç»Ÿè®¡é‡å¤§å°é€‰æ‹©ä¸åŒçš„è¿‘ä¼¼å…¬å¼
        if ad_stat_adj >= P_VALUE_THRESHOLD_HIGH:
            # é«˜ç»Ÿè®¡é‡å€¼ï¼šæŒ‡æ•°è¡°å‡å…¬å¼
            p_value = np.exp(1.2937 - 5.709*ad_stat_adj + 0.0186*(ad_stat_adj**2))
        elif ad_stat_adj >= P_VALUE_THRESHOLD_MID:
            # ä¸­ç­‰ç»Ÿè®¡é‡å€¼ï¼šä¿®æ­£æŒ‡æ•°å…¬å¼
            p_value = np.exp(0.9177 - 4.279*ad_stat_adj - 1.38*(ad_stat_adj**2))
        elif ad_stat_adj >= P_VALUE_THRESHOLD_LOW:
            # è¾ƒå°ç»Ÿè®¡é‡å€¼ï¼šäº’è¡¥æŒ‡æ•°å…¬å¼
            p_value = 1 - np.exp(-8.318 + 42.796*ad_stat_adj - 59.938*(ad_stat_adj**2))
        else:
            # æå°ç»Ÿè®¡é‡å€¼ï¼šé«˜é˜¶å¤šé¡¹å¼å…¬å¼
            p_value = 1 - np.exp(-13.436 + 101.14*ad_stat_adj - 223.73*(ad_stat_adj**2))
        
        # ç¡®ä¿på€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…
        return np.clip(p_value, 0.0, 1.0)
    
    def calculate_goodness_of_fit(self, data: np.ndarray, dist_name: str, 
                                params: Tuple[float, ...]) -> Tuple[Optional[float], Optional[float], 
                                                                   Optional[float], Optional[float]]:
        """
        è®¡ç®—åˆ†å¸ƒæ‹Ÿåˆçš„å„ç§ä¼˜åº¦æŒ‡æ ‡
        
        è®¡ç®—å¯¹æ•°ä¼¼ç„¶ã€AICã€BICå’Œä¼ªRÂ²ç­‰ç»Ÿè®¡æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°å’Œæ¯”è¾ƒä¸åŒåˆ†å¸ƒçš„æ‹Ÿåˆæ•ˆæœã€‚
        
        Args:
            data (np.ndarray): åŸå§‹æ•°æ®æ•°ç»„
            dist_name (str): åˆ†å¸ƒåç§°
            params (Tuple[float, ...]): åˆ†å¸ƒå‚æ•°
            
        Returns:
            Tuple[Optional[float], Optional[float], Optional[float], Optional[float]]: 
                (å¯¹æ•°ä¼¼ç„¶å€¼, AICå€¼, BICå€¼, ä¼ªRÂ²å€¼)ï¼Œå¤±è´¥æ—¶è¿”å›(None, None, None, None)
                
        Note:
            - AIC/BICè¶Šå°è¡¨ç¤ºæ¨¡å‹è¶Šå¥½
            - ä¼ªRÂ²è¶Šæ¥è¿‘1è¡¨ç¤ºæ‹Ÿåˆè¶Šå¥½
            - å¯¹æ•°ä¼¼ç„¶å€¼è¶Šå¤§è¡¨ç¤ºæ‹Ÿåˆè¶Šå¥½
        """
        try:
            dist = self.distributions[dist_name]
            n = len(data)
            k = len(params)  # å‚æ•°ä¸ªæ•°
            
            # ========== æ ¹æ®åˆ†å¸ƒç±»å‹è®¡ç®—å¯¹æ•°ä¼¼ç„¶ ==========
            if dist_name == 'beta':
                # Betaåˆ†å¸ƒï¼šä½¿ç”¨æ ‡å‡†åŒ–æ•°æ®
                data_range = data.max() - data.min()
                if data_range == 0:
                    return None, None, None, None
                data_scaled = (data - data.min()) / data_range
                data_scaled = np.clip(data_scaled, NUMERICAL_EPSILON, 1 - NUMERICAL_EPSILON)
                log_likelihood = np.sum(dist.logpdf(data_scaled, *params))
                
                # è®¡ç®—ç†è®ºå‡å€¼ï¼ˆè¿˜åŸåˆ°åŸå§‹å°ºåº¦ï¼‰
                theoretical_mean = dist.mean(*params) * data_range + data.min()
                ss_res = np.sum((data - theoretical_mean)**2)
                
            elif dist_name == 'lognorm':
                # å¯¹æ•°æ­£æ€åˆ†å¸ƒï¼šä½¿ç”¨åç§»æ•°æ®
                data_shifted = data + LOGNORM_SHIFT
                log_likelihood = np.sum(dist.logpdf(data_shifted, *params))
                
                # è®¡ç®—ç†è®ºå‡å€¼ï¼ˆå‡å»åç§»é‡ï¼‰
                theoretical_mean = dist.mean(*params) - LOGNORM_SHIFT
                ss_res = np.sum((data - theoretical_mean)**2)
                
            else:
                # æ ‡å‡†åˆ†å¸ƒï¼šç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®
                log_likelihood = np.sum(dist.logpdf(data, *params))
                theoretical_mean = dist.mean(*params)
                ss_res = np.sum((data - theoretical_mean)**2)
            
            # ========== è®¡ç®—æ¨¡å‹é€‰æ‹©æŒ‡æ ‡ ==========
            # AIC (Akaike Information Criterion)
            aic = 2*k - 2*log_likelihood
            
            # BIC (Bayesian Information Criterion)  
            bic = k*np.log(n) - 2*log_likelihood
            
            # ========== è®¡ç®—ä¼ªRÂ² (McFadden's RÂ²) ==========
            ss_tot = np.sum((data - np.mean(data))**2)
            r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0.0
            
            # å¤„ç†æ•°å€¼å¼‚å¸¸
            if not np.isfinite(log_likelihood):
                log_likelihood = -np.inf
            if not np.isfinite(aic):
                aic = np.inf
            if not np.isfinite(bic):
                bic = np.inf
            if not np.isfinite(r_squared):
                r_squared = 0.0
                
            return log_likelihood, aic, bic, r_squared
            
        except (ValueError, RuntimeError, np.linalg.LinAlgError) as e:
            print(f"    âŒ æ‹Ÿåˆä¼˜åº¦è®¡ç®—å¤±è´¥ ({dist_name}): {str(e)}")
            return None, None, None, None
        except Exception as e:
            print(f"    âŒ æ‹Ÿåˆä¼˜åº¦æœªçŸ¥é”™è¯¯ ({dist_name}): {str(e)}")
            return None, None, None, None
    
    def fit_variable(self, data: np.ndarray, var_name: str) -> Dict[str, Dict[str, Any]]:
        """
        å¯¹å•ä¸ªå˜é‡æ‹Ÿåˆæ‰€æœ‰æ”¯æŒçš„æ¦‚ç‡åˆ†å¸ƒ
        
        æ‰§è¡Œå®Œæ•´çš„åˆ†å¸ƒæ‹Ÿåˆåˆ†ææµç¨‹ï¼ŒåŒ…æ‹¬å‚æ•°ä¼°è®¡ã€æ‹Ÿåˆæ£€éªŒå’Œæ¨¡å‹æ¯”è¾ƒã€‚
        ä¸ºæ¯ä¸ªåˆ†å¸ƒè®¡ç®—MLEå‚æ•°ã€ADæ£€éªŒç»Ÿè®¡é‡ã€AIC/BICç­‰æŒ‡æ ‡ã€‚
        
        Args:
            data (np.ndarray): å˜é‡æ•°æ®æ•°ç»„
            var_name (str): å˜é‡åç§°ï¼Œç”¨äºç»“æœæ ‡è¯†å’Œè¾“å‡º
            
        Returns:
            Dict[str, Dict[str, Any]]: å„åˆ†å¸ƒçš„æ‹Ÿåˆç»“æœå­—å…¸
                ç»“æ„: {åˆ†å¸ƒå: {å‚æ•°, ç»Ÿè®¡é‡, æ‹ŸåˆæŒ‡æ ‡}}
                
        Note:
            - è‡ªåŠ¨è·³è¿‡æ‹Ÿåˆå¤±è´¥çš„åˆ†å¸ƒ
            - ç»“æœæŒ‰AICæ’åºï¼Œé€‰æ‹©æœ€ä½³æ‹Ÿåˆåˆ†å¸ƒ
            - æ‰€æœ‰ç»“æœä¿å­˜åˆ°self.resultsä¸­
        """
        print(f"\nğŸ¯ åˆ†æå˜é‡: {var_name}")
        print("="*60)
        
        # ========== æ•°æ®æè¿°æ€§ç»Ÿè®¡ ==========
        n_samples = len(data)
        data_mean = data.mean()
        data_std = data.std()
        data_min, data_max = data.min(), data.max()
        skewness = stats.skew(data)
        kurtosis = stats.kurtosis(data)
        
        print(f"æ•°æ®ç»Ÿè®¡: N={n_samples}, å‡å€¼={data_mean:.3f}, æ ‡å‡†å·®={data_std:.3f}")
        print(f"èŒƒå›´: [{data_min:.3f}, {data_max:.3f}]")
        print(f"ååº¦: {skewness:.3f}, å³°åº¦: {kurtosis:.3f}")
        
        # åˆå§‹åŒ–ç»“æœå­—å…¸
        results: Dict[str, Dict[str, Any]] = {}
        successful_fits = 0
        
        # ========== éå†æ‰€æœ‰åˆ†å¸ƒè¿›è¡Œæ‹Ÿåˆ ==========
        for dist_name in self.distributions.keys():
            print(f"\nğŸ“Š æ‹Ÿåˆåˆ†å¸ƒ: {dist_name}")
            
            # Step 1: MLEå‚æ•°ä¼°è®¡
            params = self.mle_estimation(data, dist_name)
            if params is None:
                continue
            
            print(f"  å‚æ•°ä¼°è®¡: {[f'{p:.4f}' for p in params]}")
            
            # Step 2: Anderson-Darlingæ‹Ÿåˆæ£€éªŒ
            ad_stat, ad_stat_adj, p_value = self.anderson_darling_test(data, dist_name, params)
            if ad_stat is None:
                continue
            
            print(f"  ADç»Ÿè®¡é‡: {ad_stat:.4f} (è°ƒæ•´å: {ad_stat_adj:.4f})")
            print(f"  på€¼: {p_value:.4f}")
            
            # Step 3: æ‹Ÿåˆä¼˜åº¦æŒ‡æ ‡è®¡ç®—
            log_like, aic, bic, r2 = self.calculate_goodness_of_fit(data, dist_name, params)
            if log_like is not None:
                print(f"  å¯¹æ•°ä¼¼ç„¶: {log_like:.4f}")
                print(f"  AIC: {aic:.4f}, BIC: {bic:.4f}")
                print(f"  ä¼ªRÂ²: {r2:.4f}")
                successful_fits += 1
            
            # Step 4: å­˜å‚¨å®Œæ•´ç»“æœ
            results[dist_name] = {
                'params': params,
                'ad_statistic': ad_stat,
                'ad_statistic_adj': ad_stat_adj, 
                'p_value': p_value,
                'log_likelihood': log_like,
                'aic': aic,
                'bic': bic,
                'r_squared': r2
            }
        
        # ========== æ¨¡å‹é€‰æ‹©ï¼šå¯»æ‰¾æœ€ä½³æ‹Ÿåˆ ==========
        if results:
            # ç­›é€‰æˆåŠŸæ‹Ÿåˆçš„åˆ†å¸ƒ
            valid_results = {k: v for k, v in results.items() 
                           if v['aic'] is not None and np.isfinite(v['aic'])}
            
            if valid_results:
                # æŒ‰AICé€‰æ‹©æœ€ä½³æ¨¡å‹
                best_dist_name = min(valid_results.keys(), 
                                   key=lambda x: valid_results[x]['aic'])
                best_aic = valid_results[best_dist_name]['aic']
                
                print(f"\nğŸ† æœ€ä½³æ‹Ÿåˆåˆ†å¸ƒ: {best_dist_name} (AIC = {best_aic:.4f})")
                print(f"âœ… æˆåŠŸæ‹Ÿåˆ {successful_fits}/{len(self.distributions)} ä¸ªåˆ†å¸ƒ")
            else:
                print(f"\nâš ï¸ æ‰€æœ‰åˆ†å¸ƒæ‹Ÿåˆå‡æœªæˆåŠŸ")
        
        # ä¿å­˜ç»“æœåˆ°å®ä¾‹å˜é‡
        self.results[var_name] = results
        return results
    
    def create_comparison_table(self) -> None:
        """
        ç”Ÿæˆå„å˜é‡åˆ†å¸ƒæ‹Ÿåˆç»“æœçš„æ±‡æ€»æ¯”è¾ƒè¡¨
        
        æŒ‰å˜é‡åˆ†ç»„æ˜¾ç¤ºæ‰€æœ‰æ‹Ÿåˆåˆ†å¸ƒçš„ç»Ÿè®¡æŒ‡æ ‡ï¼ŒåŒ…æ‹¬AICã€BICã€ADç»Ÿè®¡é‡ã€på€¼ç­‰ã€‚
        ç»“æœæŒ‰AICå‡åºæ’åˆ—ï¼Œä¾¿äºæ¨¡å‹é€‰æ‹©å’Œæ¯”è¾ƒã€‚
        
        Note:
            - è¡¨æ ¼æ˜¾ç¤ºæ ¼å¼åŒ–çš„æ•°å€¼ç»“æœ
            - æ ¹æ®på€¼(>0.05)ç»™å‡ºæ¥å—/æ‹’ç»çš„ç»“è®º
            - ç©ºå€¼å’Œæ— æ•ˆç»“æœè‡ªåŠ¨è·³è¿‡
        """
        print("\n" + "="*80)
        print("ğŸ“‹ åˆ†å¸ƒæ‹Ÿåˆç»“æœæ±‡æ€»è¡¨")
        print("="*80)
        
        for var_name, var_results in self.results.items():
            print(f"\nå˜é‡: {var_name}")
            print("-"*60)
            print(f"{'åˆ†å¸ƒ':<12} {'AIC':<8} {'BIC':<8} {'ADç»Ÿè®¡é‡':<10} {'på€¼':<8} {'ç»“è®º'}")
            print("-"*60)
            
            # æŒ‰AICå‡åºæ’åºï¼ˆè¶Šå°è¶Šå¥½ï¼‰
            sorted_results = sorted(
                var_results.items(),
                key=lambda x: x[1]['aic'] if x[1]['aic'] is not None and np.isfinite(x[1]['aic']) else float('inf')
            )
            
            # æ˜¾ç¤ºæ¯ä¸ªåˆ†å¸ƒçš„æ‹Ÿåˆç»“æœ
            for dist_name, result in sorted_results:
                aic = result.get('aic')
                bic = result.get('bic')
                ad_stat = result.get('ad_statistic_adj')
                p_val = result.get('p_value')
                
                # åªæ˜¾ç¤ºå®Œæ•´æœ‰æ•ˆçš„ç»“æœ
                if all(x is not None and np.isfinite(x) for x in [aic, bic, ad_stat, p_val]):
                    # å‡è®¾æ£€éªŒç»“è®ºï¼ˆH0ï¼šæ•°æ®ç¬¦åˆè¯¥åˆ†å¸ƒï¼‰
                    conclusion = "æ¥å—" if p_val > 0.05 else "æ‹’ç»"
                    print(f"{dist_name:<12} {aic:<8.2f} {bic:<8.2f} {ad_stat:<10.4f} {p_val:<8.4f} {conclusion}")


def main() -> Optional[DistributionFitter]:
    """
    ä¸»ç¨‹åºå…¥å£ï¼šæ‰§è¡Œå®Œæ•´çš„å†œæ‘å¥³æ€§å°±ä¸šå¸‚åœºæ•°æ®åˆ†å¸ƒæ¨æ–­åˆ†æ
    
    ç¨‹åºæµç¨‹ï¼š
    1. åŠ è½½æ¸…æ´—åçš„æ•°æ®æ–‡ä»¶
    2. åˆ›å»ºå¤åˆçŠ¶æ€å˜é‡ï¼ˆå¦‚æ¯å‘¨å·¥ä½œæ—¶é•¿ï¼‰
    3. å®šä¹‰å…³é”®å˜é‡ï¼ˆçŠ¶æ€å˜é‡+æ§åˆ¶å˜é‡ï¼‰
    4. å¯¹æ¯ä¸ªå˜é‡è¿›è¡Œå¤šåˆ†å¸ƒæ‹Ÿåˆåˆ†æ
    5. ç”Ÿæˆæ±‡æ€»æ¯”è¾ƒè¡¨
    
    Returns:
        Optional[DistributionFitter]: æˆåŠŸæ—¶è¿”å›æ‹Ÿåˆå™¨å¯¹è±¡ï¼Œå¤±è´¥æ—¶è¿”å›None
        
    Note:
        - æ•°æ®æ–‡ä»¶åº”ä¸ºUTF-8ç¼–ç çš„CSVæ ¼å¼
        - å˜é‡å®šä¹‰ä¸¥æ ¼å¯¹åº”ç ”ç©¶è®¡åˆ’ç¬¬4.2èŠ‚
        - è‡ªåŠ¨è·³è¿‡æ•°æ®ç‚¹ä¸è¶³(N<10)çš„å˜é‡
    """
    print("ğŸ” å†œæ‘å¥³æ€§å°±ä¸šå¸‚åœºä¸»ä½“ç‰¹å¾åˆ†å¸ƒæ¨æ–­")
    print("åŸºäºMLEå‚æ•°ä¼°è®¡ä¸Anderson-Darlingæ£€éªŒ")
    print("="*60)
    
    # ========== Step 1: æ•°æ®åŠ è½½ä¸éªŒè¯ ==========
    try:
        df = pd.read_csv("cleaned_data.csv", encoding='utf-8-sig')
        print(f"âœ“ æˆåŠŸåŠ è½½æ•°æ®ï¼š{df.shape[0]}ä¸ªæ ·æœ¬ï¼Œ{df.shape[1]}ä¸ªå˜é‡")
        
        # åŸºæœ¬æ•°æ®è´¨é‡æ£€æŸ¥
        if df.shape[0] < MIN_SAMPLE_SIZE:
            print(f"âŒ æ ·æœ¬é‡ä¸è¶³ï¼šéœ€è¦è‡³å°‘{MIN_SAMPLE_SIZE}ä¸ªæ ·æœ¬")
            return None
            
    except FileNotFoundError:
        print("âŒ æ•°æ®æ–‡ä»¶ 'cleaned_data.csv' æœªæ‰¾åˆ°")
        return None
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None
    
    # ========== Step 2: åˆ›å»ºåˆ†å¸ƒæ‹Ÿåˆå™¨ ==========
    fitter = DistributionFitter()
    
    # ========== Step 3: æ„é€ å¤åˆçŠ¶æ€å˜é‡ ==========
    # T = å·¥ä½œæ—¶é—´æŠ•å…¥ = æ¯å‘¨æœŸæœ›å·¥ä½œå¤©æ•° Ã— æ¯å¤©æœŸæœ›å·¥ä½œæ—¶æ•°
    df['æ¯å‘¨å·¥ä½œæ—¶é•¿'] = df['æ¯å‘¨æœŸæœ›å·¥ä½œå¤©æ•°'] * df['æ¯å¤©æœŸæœ›å·¥ä½œæ—¶æ•°']
    print("âœ“ åˆ›å»ºå¤åˆçŠ¶æ€å˜é‡ï¼šæ¯å‘¨å·¥ä½œæ—¶é•¿ = å·¥ä½œå¤©æ•° Ã— å·¥ä½œæ—¶æ•°")
    
    # ========== Step 4: å®šä¹‰åˆ†æå˜é‡é›†åˆ ==========
    # ä¸¥æ ¼æŒ‰ç…§ç ”ç©¶è®¡åˆ’ç¬¬4.2èŠ‚å®šä¹‰çš„å˜é‡ä½“ç³»
    key_variables = {
        # ===== æ ¸å¿ƒçŠ¶æ€å˜é‡ x = (T, S, D, W) =====
        'æ¯å‘¨å·¥ä½œæ—¶é•¿': df['æ¯å‘¨å·¥ä½œæ—¶é•¿'],            # T - å·¥ä½œæ—¶é—´æŠ•å…¥ï¼ˆå¤åˆå˜é‡ï¼‰
        'å·¥ä½œèƒ½åŠ›è¯„åˆ†': df['å·¥ä½œèƒ½åŠ›è¯„åˆ†'],            # S - å·¥ä½œèƒ½åŠ›æ°´å¹³  
        'æ•°å­—ç´ å…»è¯„åˆ†': df['æ•°å­—ç´ å…»è¯„åˆ†'],            # D - æ•°å­—ç´ å…»
        'æ¯æœˆæœŸæœ›æ”¶å…¥': df['æ¯æœˆæœŸæœ›æ”¶å…¥'],            # W - æœŸæœ›å·¥ä½œå¾…é‡
        
        # ===== æ§åˆ¶å˜é‡ Ïƒ =====
        'å¹´é¾„': df['å¹´é¾„'],                          # äººå£ç»Ÿè®¡å­¦æ§åˆ¶å˜é‡
        'ç´¯è®¡å·¥ä½œå¹´é™': df['ç´¯è®¡å·¥ä½œå¹´é™'],            # å·¥ä½œç»éªŒæ§åˆ¶å˜é‡
        'å®¶åŠ¡åŠ³åŠ¨æ—¶é—´': df['å®¶åŠ¡åŠ³åŠ¨æ—¶é—´'],            # æ—¶é—´é…ç½®æ§åˆ¶å˜é‡
        'é—²æš‡æ—¶é—´': df['é—²æš‡æ—¶é—´'],                   # æ—¶é—´é…ç½®æ§åˆ¶å˜é‡
        
        # ===== åŸå§‹æ„æˆå˜é‡ï¼ˆç”¨äºéªŒè¯ï¼‰ =====
        'æ¯å‘¨æœŸæœ›å·¥ä½œå¤©æ•°': df['æ¯å‘¨æœŸæœ›å·¥ä½œå¤©æ•°'],    # Tçš„æ„æˆè¦ç´ 1
        'æ¯å¤©æœŸæœ›å·¥ä½œæ—¶æ•°': df['æ¯å¤©æœŸæœ›å·¥ä½œæ—¶æ•°']     # Tçš„æ„æˆè¦ç´ 2
    }
    
    print(f"âœ“ å®šä¹‰{len(key_variables)}ä¸ªå…³é”®å˜é‡å¾…åˆ†æ")
    
    # ========== Step 5: æ‰¹é‡åˆ†å¸ƒæ‹Ÿåˆåˆ†æ ==========
    analyzed_count = 0
    skipped_count = 0
    
    for var_name, data in key_variables.items():
        # æ•°æ®é¢„å¤„ç†ï¼šç§»é™¤ç¼ºå¤±å€¼
        clean_data = data.dropna()
        
        # æ ·æœ¬é‡æ£€æŸ¥
        if len(clean_data) < MIN_SAMPLE_SIZE:
            print(f"âš ï¸  å˜é‡ '{var_name}' æ•°æ®ç‚¹ä¸è¶³({len(clean_data)}<{MIN_SAMPLE_SIZE})ï¼Œè·³è¿‡åˆ†æ")
            skipped_count += 1
            continue
        
        # æ‰§è¡Œåˆ†å¸ƒæ‹Ÿåˆåˆ†æ
        fitter.fit_variable(clean_data.values, var_name)
        analyzed_count += 1
    
    # ========== Step 6: ç”Ÿæˆåˆ†ææŠ¥å‘Š ==========
    if analyzed_count > 0:
        fitter.create_comparison_table()
        
        print(f"\nâœ… åˆ†å¸ƒæ¨æ–­åˆ†æå®Œæˆ!")
        print(f"ğŸ“Š æˆåŠŸåˆ†æ: {analyzed_count} ä¸ªå˜é‡")
        if skipped_count > 0:
            print(f"âš ï¸  è·³è¿‡: {skipped_count} ä¸ªå˜é‡ï¼ˆæ•°æ®ä¸è¶³ï¼‰")
        print("="*60)
        
        return fitter
    else:
        print("\nâŒ æ²¡æœ‰å˜é‡æ»¡è¶³åˆ†ææ¡ä»¶")
        return None

if __name__ == "__main__":
    fitter = main()
